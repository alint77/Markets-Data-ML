{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.utils as utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import ta\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "csvFileAddress = input('csv file address') \n",
    "\n",
    "DJ = pd.read_csv(csvFileAddress,delim_whitespace=True)\n",
    "\n",
    "DJ['<ISGREEN>'] =  DJ['<CLOSE>'] > DJ['<OPEN>']\n",
    "DJ['<SIZE>'] =  (DJ['<CLOSE>'] - DJ['<OPEN>']) / DJ['<CLOSE>']\n",
    "DJ['<VOLATILITY>'] =  DJ['<HIGH>'] - DJ['<LOW>'] /DJ['<CLOSE>']\n",
    "\n",
    "DJ.drop(['<VOL>'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "timeFrame = (int(DJ.iloc[1][1][1])-int(DJ.iloc[0][1][1]) ) * 60 + (int(DJ.iloc[1][1][3:5])-int(DJ.iloc[0][1][3:5]))\n",
    "CandlesInDay = 24 * (60//timeFrame)\n",
    "timeFrame\n",
    "def candleToTime(j):\n",
    "    minuteMult = CandlesInDay//24\n",
    "    k=j//minuteMult\n",
    "    sth=timeFrame*(j%minuteMult)\n",
    "    return '{:02d}:{:02d}:00'.format(k,sth)\n",
    "\n",
    "uniqueDays = DJ.drop_duplicates(subset='<DATE>')\n",
    "uniqueDays = pd.DataFrame(uniqueDays)\n",
    "\n",
    "uniqueDaysCount=uniqueDays.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "newnumparr = np.full((uniqueDaysCount*CandlesInDay,2),'',dtype=np.object_)\n",
    "\n",
    "\n",
    "for i in range(uniqueDaysCount):\n",
    "    for j in range(CandlesInDay):\n",
    "        newnumparr[(i*CandlesInDay)+j]=[uniqueDays.iloc[i][0],candleToTime(j)]\n",
    "\n",
    "newDF = pd.DataFrame(newnumparr,columns=['<DATE>','<TIME>'])\n",
    "\n",
    "newestDF = newDF.merge(DJ,on=['<DATE>','<TIME>'],how='left')\n",
    "\n",
    "# newestDF.drop(columns=['<OPEN>_x','<CLOSE>_x','<SIZE>_x','<VOLATILITY>_x','<ISGREEN>_x'],inplace=True)\n",
    "newestDF.fillna(0,inplace=True)\n",
    "\n",
    "newestDF['<EMA30>']= ta.trend.ema_indicator( newestDF['<CLOSE>'],window=30)\n",
    "newestDF['<EMA50>']= ta.trend.ema_indicator( newestDF['<CLOSE>'],window=50)\n",
    "newestDF['<EMA200>']= ta.trend.ema_indicator( newestDF['<CLOSE>'],window=200)\n",
    "\n",
    "def Upper(e):\n",
    "    if e['<ISGREEN>'] : \n",
    "        return e['<HIGH>']-e['<CLOSE>']\n",
    "    return e['<HIGH>']-e['<OPEN>']\n",
    "\n",
    "def Lower(e):\n",
    "    if e['<ISGREEN>'] : \n",
    "        return e['<OPEN>']-e['<LOW>']\n",
    "    return e['<CLOSE>']-e['<LOW>']\n",
    "\n",
    "\n",
    "newestDF['<UPPER>'] = newestDF.apply(Upper,axis=1)\n",
    "newestDF['<LOWER>'] = newestDF.apply(Lower,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Compute RSI\n",
    "\n",
    "newestDF['<RSI>'] = ta.momentum.rsi(newestDF['<CLOSE>'],window=15,fillna=1) / 100\n",
    "newestDF['<CCI>'] = ta.trend.cci(close=newestDF['<CLOSE>'],high=newestDF['<HIGH>'],low=newestDF['<LOW>'],window=14,fillna=1)\n",
    "\n",
    "\n",
    "keltner = ta.volatility.KeltnerChannel(close=newestDF['<CLOSE>'],high=newestDF['<HIGH>'],low=newestDF['<LOW>'],window=20,window_atr=10,multiplier=2.5,fillna=1)\n",
    "newestDF['<KELTNER_H>'] = keltner.keltner_channel_hband()\n",
    "newestDF['<KELTNER_L>'] = keltner.keltner_channel_lband()\n",
    "newestDF['<KELTNER_M>'] = keltner.keltner_channel_mband()\n",
    "\n",
    "\n",
    "newestDF['<ATR_24>'] = ta.volatility.average_true_range(close=newestDF['<CLOSE>'],high=newestDF['<HIGH>'],low=newestDF['<LOW>'],window=30).apply(lambda e : e/100)\n",
    "\n",
    "\n",
    "newestDF[\"<GREEN>\"] = newestDF[\"<ISGREEN>\"].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (newestDF,open=\"<OPEN>\",close='<CLOSE>',high='<HIGH>',low='<LOW>',volume='<TICKVOL>',)\n",
    "uniqueDaysCount\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;DATE&gt;</th>\n",
       "      <th>&lt;TIME&gt;</th>\n",
       "      <th>&lt;OPEN&gt;</th>\n",
       "      <th>&lt;HIGH&gt;</th>\n",
       "      <th>&lt;LOW&gt;</th>\n",
       "      <th>&lt;CLOSE&gt;</th>\n",
       "      <th>&lt;TICKVOL&gt;</th>\n",
       "      <th>&lt;SPREAD&gt;</th>\n",
       "      <th>&lt;ISGREEN&gt;</th>\n",
       "      <th>&lt;SIZE&gt;</th>\n",
       "      <th>&lt;VOLATILITY&gt;</th>\n",
       "      <th>&lt;EMA30&gt;</th>\n",
       "      <th>&lt;EMA50&gt;</th>\n",
       "      <th>&lt;EMA200&gt;</th>\n",
       "      <th>&lt;UPPER&gt;</th>\n",
       "      <th>&lt;LOWER&gt;</th>\n",
       "      <th>&lt;RSI&gt;</th>\n",
       "      <th>&lt;CCI&gt;</th>\n",
       "      <th>&lt;KELTNER_H&gt;</th>\n",
       "      <th>&lt;KELTNER_L&gt;</th>\n",
       "      <th>&lt;KELTNER_M&gt;</th>\n",
       "      <th>&lt;ATR_24&gt;</th>\n",
       "      <th>&lt;GREEN&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019.04.22</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019.04.22</td>\n",
       "      <td>00:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019.04.22</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019.04.22</td>\n",
       "      <td>01:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019.04.22</td>\n",
       "      <td>02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       <DATE>    <TIME>  <OPEN>  ...  <KELTNER_M>  <ATR_24>  <GREEN>\n",
       "0  2019.04.22  00:00:00     0.0  ...          0.0       0.0        0\n",
       "1  2019.04.22  00:30:00     0.0  ...          0.0       0.0        0\n",
       "2  2019.04.22  01:00:00     0.0  ...          0.0       0.0        0\n",
       "3  2019.04.22  01:30:00     0.0  ...          0.0       0.0        0\n",
       "4  2019.04.22  02:00:00     0.0  ...          0.0       0.0        0\n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newestDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input_close = closingPriceOnly =  newestDF[['<CLOSE>']].iloc[:-24].to_numpy()\n",
    "# target_close = closingPriceOnly = newestDF[['<CLOSE>']].iloc[17:].to_numpy()\n",
    "\n",
    "# dataset = utils.timeseries_dataset_from_array(input_close,targets= target_close,sequence_length=17,sequence_stride=24,batch_size=32,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(list(dataset.as_numpy_iterator()),dtype=object).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeFrame = (int(newestDF.iloc[1]['<TIME>'][1])-int(newestDF.iloc[0]['<TIME>'][1]) ) * 60 + (int(newestDF.iloc[1]['<TIME>'][3:5])-int(newestDF.iloc[0]['<TIME>'][3:5]))\n",
    "CandlesInDay = 24 * (60//timeFrame)\n",
    "uniqueDaysCount = len(newestDF)//CandlesInDay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "startCandleHour = 4\n",
    "startCandleIndex = int(startCandleHour * CandlesInDay )// 24\n",
    "\n",
    "windowLengthHour = 13\n",
    "windowLengthCount = int(windowLengthHour * CandlesInDay) // 24\n",
    "\n",
    "\n",
    "labelStartCandleHour = startCandleHour+windowLengthHour\n",
    "labelStartCandleIndex = int(labelStartCandleHour * CandlesInDay) // 24\n",
    "\n",
    "labelWindowLengthHour = 5\n",
    "labelWindowLengthCount = int(labelWindowLengthHour * CandlesInDay) // 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "takeProfitPoints = 1.003\n",
    "stopLoss = .997\n",
    "PriceOnly = newestDF.drop(['<DATE>','<TIME>'],axis=1)\n",
    "PriceOnly\n",
    "windows = []\n",
    "labelsDFs = []\n",
    "labelValues = []\n",
    "df = PriceOnly\n",
    "\n",
    "maxValues = df.max()\n",
    "\n",
    "for i in range(0,len(df)-CandlesInDay , CandlesInDay):\n",
    "    trainDF = df.iloc[i+startCandleIndex : (i+startCandleIndex +windowLengthCount)]\n",
    "    labelDF = df.iloc[i+labelStartCandleIndex :(i+labelStartCandleIndex+labelWindowLengthCount) ]\n",
    "\n",
    "    lowestIndex = labelDF['<LOW>'].idxmin()\n",
    "    lowestPriceInLabels = labelDF['<LOW>'][lowestIndex]\n",
    "\n",
    "    highesIndex = labelDF['<HIGH>'].idxmax()\n",
    "    highestPriceInLabels = labelDF['<HIGH>'][highesIndex]\n",
    "\n",
    "    TP = labelDF['<OPEN>'].iloc[0]*takeProfitPoints\n",
    "    SL = labelDF['<OPEN>'].iloc[0]*stopLoss\n",
    "\n",
    "    buySignal = 0\n",
    "\n",
    "    if highestPriceInLabels < TP and lowestPriceInLabels > SL:\n",
    "        # ideas : halfTP\n",
    "        buySignal=2\n",
    "        \n",
    "    elif highestPriceInLabels < TP and lowestPriceInLabels < SL:\n",
    "        buySignal = 0\n",
    "    elif highestPriceInLabels > TP and lowestPriceInLabels > SL:\n",
    "        buySignal = 1\n",
    "    else :\n",
    "        tpIndex = 0\n",
    "        slIndex = 0\n",
    "        for j in range(len(labelDF)):\n",
    "            if labelDF['<HIGH>'].iloc[j] > TP and labelDF['<LOW>'].iloc[j]>SL :\n",
    "                tpIndex=j\n",
    "            if labelDF['<HIGH>'].iloc[j] < TP and labelDF['<LOW>'].iloc[j]>SL :\n",
    "                continue\n",
    "            if labelDF['<HIGH>'].iloc[j] < TP and labelDF['<LOW>'].iloc[j]<SL :\n",
    "                slIndex=j\n",
    "        buySignal = int(tpIndex<slIndex)\n",
    "        if slIndex==tpIndex:\n",
    "            continue\n",
    "    \n",
    "\n",
    "    # trainDF =  trainDF /df.abs().max()\n",
    "    trainDF=(trainDF-df.min())/(df.max()-df.min())\n",
    "\n",
    "    # trainDF = trainDF.drop(['<OPEN>','<ATR_24>','<EMA30>','<RSI>','<CCI>','<KELTNER_M>','<KELTNER_L>','<KELTNER_H>','<GREEN>'],axis=1)\n",
    "    trainDF = trainDF[['<SIZE>','<UPPER>','<LOWER>','<TICKVOL>','<CCI>','<RSI>','<KELTNER_M>','<KELTNER_L>','<KELTNER_H>']]\n",
    "\n",
    "\n",
    "    trainNp =(trainDF.to_numpy(dtype=np.float32))\n",
    "    # trainNp = np.rot90( trainDF.to_numpy())\n",
    "\n",
    "\n",
    "    labelValues.append(buySignal)\n",
    "    windows.append(trainNp)\n",
    "    labelsDFs.append(labelDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "signal\n",
       "0         362\n",
       "1         305\n",
       "2         114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(labelValues,columns=['signal']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>0.539314</td>\n",
       "      <td>0.539183</td>\n",
       "      <td>0.539362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.387270</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.019390</td>\n",
       "      <td>0.555793</td>\n",
       "      <td>0.566950</td>\n",
       "      <td>0.539371</td>\n",
       "      <td>0.539223</td>\n",
       "      <td>0.539437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.381948</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.016558</td>\n",
       "      <td>0.555854</td>\n",
       "      <td>0.567087</td>\n",
       "      <td>0.539464</td>\n",
       "      <td>0.539306</td>\n",
       "      <td>0.539541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.023312</td>\n",
       "      <td>0.555913</td>\n",
       "      <td>0.567298</td>\n",
       "      <td>0.539548</td>\n",
       "      <td>0.539408</td>\n",
       "      <td>0.539605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.372204</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.028214</td>\n",
       "      <td>0.555845</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>0.539644</td>\n",
       "      <td>0.539540</td>\n",
       "      <td>0.539666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.376189</td>\n",
       "      <td>0.025559</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.020697</td>\n",
       "      <td>0.555768</td>\n",
       "      <td>0.567195</td>\n",
       "      <td>0.539748</td>\n",
       "      <td>0.539655</td>\n",
       "      <td>0.539759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.378402</td>\n",
       "      <td>0.031949</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>0.028867</td>\n",
       "      <td>0.555731</td>\n",
       "      <td>0.567285</td>\n",
       "      <td>0.539845</td>\n",
       "      <td>0.539744</td>\n",
       "      <td>0.539864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.380171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.025272</td>\n",
       "      <td>0.555695</td>\n",
       "      <td>0.567396</td>\n",
       "      <td>0.539952</td>\n",
       "      <td>0.539871</td>\n",
       "      <td>0.539951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.377074</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.011331</td>\n",
       "      <td>0.019063</td>\n",
       "      <td>0.555699</td>\n",
       "      <td>0.567440</td>\n",
       "      <td>0.540089</td>\n",
       "      <td>0.540021</td>\n",
       "      <td>0.540076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.372205</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.555634</td>\n",
       "      <td>0.567260</td>\n",
       "      <td>0.540240</td>\n",
       "      <td>0.540169</td>\n",
       "      <td>0.540229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.381055</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.550107</td>\n",
       "      <td>0.567448</td>\n",
       "      <td>0.540418</td>\n",
       "      <td>0.540368</td>\n",
       "      <td>0.540386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.545602</td>\n",
       "      <td>0.567484</td>\n",
       "      <td>0.540595</td>\n",
       "      <td>0.540578</td>\n",
       "      <td>0.540530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.374861</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.541849</td>\n",
       "      <td>0.567429</td>\n",
       "      <td>0.540741</td>\n",
       "      <td>0.540927</td>\n",
       "      <td>0.540474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.377516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.538747</td>\n",
       "      <td>0.567513</td>\n",
       "      <td>0.540855</td>\n",
       "      <td>0.541125</td>\n",
       "      <td>0.540505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.373091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011331</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>0.568064</td>\n",
       "      <td>0.567354</td>\n",
       "      <td>0.540944</td>\n",
       "      <td>0.541247</td>\n",
       "      <td>0.540561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.377516</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.608916</td>\n",
       "      <td>0.567450</td>\n",
       "      <td>0.541054</td>\n",
       "      <td>0.541418</td>\n",
       "      <td>0.540610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.390768</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.020697</td>\n",
       "      <td>0.736994</td>\n",
       "      <td>0.568297</td>\n",
       "      <td>0.577172</td>\n",
       "      <td>0.577535</td>\n",
       "      <td>0.576723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.378838</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.021678</td>\n",
       "      <td>0.781353</td>\n",
       "      <td>0.568462</td>\n",
       "      <td>0.613314</td>\n",
       "      <td>0.613704</td>\n",
       "      <td>0.612832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.385456</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.031264</td>\n",
       "      <td>0.753870</td>\n",
       "      <td>0.569137</td>\n",
       "      <td>0.649482</td>\n",
       "      <td>0.649879</td>\n",
       "      <td>0.648988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.370006</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.669502</td>\n",
       "      <td>0.568556</td>\n",
       "      <td>0.685647</td>\n",
       "      <td>0.686067</td>\n",
       "      <td>0.685124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.390300</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.666480</td>\n",
       "      <td>0.569663</td>\n",
       "      <td>0.721833</td>\n",
       "      <td>0.722257</td>\n",
       "      <td>0.721302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.385441</td>\n",
       "      <td>0.025559</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>0.686955</td>\n",
       "      <td>0.570415</td>\n",
       "      <td>0.722051</td>\n",
       "      <td>0.722477</td>\n",
       "      <td>0.721518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.026471</td>\n",
       "      <td>0.681729</td>\n",
       "      <td>0.571637</td>\n",
       "      <td>0.722278</td>\n",
       "      <td>0.722684</td>\n",
       "      <td>0.721764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.379706</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.023203</td>\n",
       "      <td>0.668587</td>\n",
       "      <td>0.572004</td>\n",
       "      <td>0.722499</td>\n",
       "      <td>0.722940</td>\n",
       "      <td>0.721951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.372225</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.634068</td>\n",
       "      <td>0.571438</td>\n",
       "      <td>0.722713</td>\n",
       "      <td>0.723172</td>\n",
       "      <td>0.722147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.367377</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.591841</td>\n",
       "      <td>0.570095</td>\n",
       "      <td>0.722906</td>\n",
       "      <td>0.723363</td>\n",
       "      <td>0.722342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2  ...         6         7         8\n",
       "0   0.375746  0.000000  0.000000  ...  0.539314  0.539183  0.539362\n",
       "1   0.387270  0.012780  0.007082  ...  0.539371  0.539223  0.539437\n",
       "2   0.381948  0.007987  0.004249  ...  0.539464  0.539306  0.539541\n",
       "3   0.384600  0.028754  0.008499  ...  0.539548  0.539408  0.539605\n",
       "4   0.372204  0.022364  0.002833  ...  0.539644  0.539540  0.539666\n",
       "5   0.376189  0.025559  0.004249  ...  0.539748  0.539655  0.539759\n",
       "6   0.378402  0.031949  0.015581  ...  0.539845  0.539744  0.539864\n",
       "7   0.380171  0.000000  0.007082  ...  0.539952  0.539871  0.539951\n",
       "8   0.377074  0.015974  0.011331  ...  0.540089  0.540021  0.540076\n",
       "9   0.372205  0.011182  0.002833  ...  0.540240  0.540169  0.540229\n",
       "10  0.381055  0.011182  0.001416  ...  0.540418  0.540368  0.540386\n",
       "11  0.375746  0.007987  0.007082  ...  0.540595  0.540578  0.540530\n",
       "12  0.374861  0.006390  0.001416  ...  0.540741  0.540927  0.540474\n",
       "13  0.377516  0.000000  0.009915  ...  0.540855  0.541125  0.540505\n",
       "14  0.373091  0.000000  0.011331  ...  0.540944  0.541247  0.540561\n",
       "15  0.377516  0.019169  0.001416  ...  0.541054  0.541418  0.540610\n",
       "16  0.390768  0.001597  0.007082  ...  0.577172  0.577535  0.576723\n",
       "17  0.378838  0.012780  0.005666  ...  0.613314  0.613704  0.612832\n",
       "18  0.385456  0.015974  0.002833  ...  0.649482  0.649879  0.648988\n",
       "19  0.370006  0.011182  0.002833  ...  0.685647  0.686067  0.685124\n",
       "20  0.390300  0.003195  0.002833  ...  0.721833  0.722257  0.721302\n",
       "21  0.385441  0.025559  0.000000  ...  0.722051  0.722477  0.721518\n",
       "22  0.389831  0.004792  0.001416  ...  0.722278  0.722684  0.721764\n",
       "23  0.379706  0.011182  0.004249  ...  0.722499  0.722940  0.721951\n",
       "24  0.372225  0.004792  0.000000  ...  0.722713  0.723172  0.722147\n",
       "25  0.367377  0.004792  0.000000  ...  0.722906  0.723363  0.722342\n",
       "\n",
       "[26 rows x 9 columns]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(windows[43])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(781, 781, 783)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labelValues),len(labelsDFs),uniqueDaysCount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(windows)\n",
    "windows2= np.array(windows)\n",
    "# windows=windows[...,np.newaxis]\n",
    "\n",
    "labelValues = np.array(labelValues)\n",
    "\n",
    "\n",
    "train_data = windows2[:int(l*.75)]\n",
    "val_data = windows2[int(l*.75):]\n",
    "\n",
    "\n",
    "\n",
    "train_labels = labelValues[:int(l*.75)].astype(np.uint8)\n",
    "train_labels_hot = np.eye(3).astype(np.uint8)[train_labels]\n",
    "\n",
    "\n",
    "val_labels = labelValues[int(l*.75):].astype(np.uint8)\n",
    "val_labels_hot = (np.eye(3).astype(np.uint8)[val_labels])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 26, 9)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.540336</td>\n",
       "      <td>0.540218</td>\n",
       "      <td>0.540372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.376629</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.012748</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.556026</td>\n",
       "      <td>0.567359</td>\n",
       "      <td>0.540379</td>\n",
       "      <td>0.540297</td>\n",
       "      <td>0.540380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.376629</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.555961</td>\n",
       "      <td>0.567369</td>\n",
       "      <td>0.540432</td>\n",
       "      <td>0.540366</td>\n",
       "      <td>0.540417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.381924</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.011329</td>\n",
       "      <td>0.555931</td>\n",
       "      <td>0.567527</td>\n",
       "      <td>0.540575</td>\n",
       "      <td>0.540608</td>\n",
       "      <td>0.540460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.377511</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.012418</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.567583</td>\n",
       "      <td>0.540775</td>\n",
       "      <td>0.540843</td>\n",
       "      <td>0.540627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.380598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>0.012309</td>\n",
       "      <td>0.555853</td>\n",
       "      <td>0.567691</td>\n",
       "      <td>0.541004</td>\n",
       "      <td>0.541119</td>\n",
       "      <td>0.540807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.370892</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.015577</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.567490</td>\n",
       "      <td>0.541277</td>\n",
       "      <td>0.541493</td>\n",
       "      <td>0.540980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.383242</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.015468</td>\n",
       "      <td>0.555764</td>\n",
       "      <td>0.567739</td>\n",
       "      <td>0.541492</td>\n",
       "      <td>0.541764</td>\n",
       "      <td>0.541139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.379273</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.015795</td>\n",
       "      <td>0.555763</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>0.541712</td>\n",
       "      <td>0.541996</td>\n",
       "      <td>0.541348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.378390</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.555713</td>\n",
       "      <td>0.567936</td>\n",
       "      <td>0.541946</td>\n",
       "      <td>0.542253</td>\n",
       "      <td>0.541558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.373983</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>0.550165</td>\n",
       "      <td>0.567840</td>\n",
       "      <td>0.542158</td>\n",
       "      <td>0.542486</td>\n",
       "      <td>0.541749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.373542</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.545579</td>\n",
       "      <td>0.567711</td>\n",
       "      <td>0.542335</td>\n",
       "      <td>0.542713</td>\n",
       "      <td>0.541876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.373541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.541790</td>\n",
       "      <td>0.567601</td>\n",
       "      <td>0.542496</td>\n",
       "      <td>0.542910</td>\n",
       "      <td>0.542002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.378391</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.538721</td>\n",
       "      <td>0.567726</td>\n",
       "      <td>0.542658</td>\n",
       "      <td>0.543097</td>\n",
       "      <td>0.542139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.374423</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.559277</td>\n",
       "      <td>0.567631</td>\n",
       "      <td>0.542818</td>\n",
       "      <td>0.543273</td>\n",
       "      <td>0.542282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.377950</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.010240</td>\n",
       "      <td>0.551498</td>\n",
       "      <td>0.567726</td>\n",
       "      <td>0.542965</td>\n",
       "      <td>0.543490</td>\n",
       "      <td>0.542360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.377509</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.587745</td>\n",
       "      <td>0.567855</td>\n",
       "      <td>0.579188</td>\n",
       "      <td>0.579753</td>\n",
       "      <td>0.578538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.373101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0.540462</td>\n",
       "      <td>0.567582</td>\n",
       "      <td>0.615403</td>\n",
       "      <td>0.616009</td>\n",
       "      <td>0.614707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.377509</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.016340</td>\n",
       "      <td>0.546296</td>\n",
       "      <td>0.567700</td>\n",
       "      <td>0.651620</td>\n",
       "      <td>0.652258</td>\n",
       "      <td>0.650886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.377509</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.600137</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>0.687843</td>\n",
       "      <td>0.688521</td>\n",
       "      <td>0.687064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.373101</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>0.560714</td>\n",
       "      <td>0.567523</td>\n",
       "      <td>0.724064</td>\n",
       "      <td>0.724777</td>\n",
       "      <td>0.723243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.377068</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.567631</td>\n",
       "      <td>0.724129</td>\n",
       "      <td>0.724852</td>\n",
       "      <td>0.723299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.372660</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.428347</td>\n",
       "      <td>0.567247</td>\n",
       "      <td>0.724185</td>\n",
       "      <td>0.724902</td>\n",
       "      <td>0.723360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.371335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.285579</td>\n",
       "      <td>0.566661</td>\n",
       "      <td>0.724215</td>\n",
       "      <td>0.724944</td>\n",
       "      <td>0.723380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.377951</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>0.293796</td>\n",
       "      <td>0.566839</td>\n",
       "      <td>0.724232</td>\n",
       "      <td>0.724968</td>\n",
       "      <td>0.723390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.366478</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.018192</td>\n",
       "      <td>0.235084</td>\n",
       "      <td>0.565363</td>\n",
       "      <td>0.724227</td>\n",
       "      <td>0.724959</td>\n",
       "      <td>0.723389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2  ...         6         7         8\n",
       "0   0.375746  0.000000  0.000000  ...  0.540336  0.540218  0.540372\n",
       "1   0.376629  0.009585  0.012748  ...  0.540379  0.540297  0.540380\n",
       "2   0.376629  0.004792  0.004249  ...  0.540432  0.540366  0.540417\n",
       "3   0.381924  0.001597  0.007082  ...  0.540575  0.540608  0.540460\n",
       "4   0.377511  0.006390  0.009915  ...  0.540775  0.540843  0.540627\n",
       "5   0.380598  0.000000  0.014164  ...  0.541004  0.541119  0.540807\n",
       "6   0.370892  0.003195  0.002833  ...  0.541277  0.541493  0.540980\n",
       "7   0.383242  0.003195  0.002833  ...  0.541492  0.541764  0.541139\n",
       "8   0.379273  0.014377  0.002833  ...  0.541712  0.541996  0.541348\n",
       "9   0.378390  0.006390  0.001416  ...  0.541946  0.542253  0.541558\n",
       "10  0.373983  0.012780  0.000000  ...  0.542158  0.542486  0.541749\n",
       "11  0.373542  0.001597  0.004249  ...  0.542335  0.542713  0.541876\n",
       "12  0.373541  0.000000  0.000000  ...  0.542496  0.542910  0.542002\n",
       "13  0.378391  0.003195  0.001416  ...  0.542658  0.543097  0.542139\n",
       "14  0.374423  0.007987  0.004249  ...  0.542818  0.543273  0.542282\n",
       "15  0.377950  0.001597  0.004249  ...  0.542965  0.543490  0.542360\n",
       "16  0.377509  0.007987  0.001416  ...  0.579188  0.579753  0.578538\n",
       "17  0.373101  0.000000  0.004249  ...  0.615403  0.616009  0.614707\n",
       "18  0.377509  0.007987  0.008499  ...  0.651620  0.652258  0.650886\n",
       "19  0.377509  0.003195  0.005666  ...  0.687843  0.688521  0.687064\n",
       "20  0.373101  0.007987  0.002833  ...  0.724064  0.724777  0.723243\n",
       "21  0.377068  0.006390  0.004249  ...  0.724129  0.724852  0.723299\n",
       "22  0.372660  0.001597  0.005666  ...  0.724185  0.724902  0.723360\n",
       "23  0.371335  0.000000  0.002833  ...  0.724215  0.724944  0.723380\n",
       "24  0.377951  0.001597  0.005666  ...  0.724232  0.724968  0.723390\n",
       "25  0.366478  0.003195  0.001416  ...  0.724227  0.724959  0.723389\n",
       "\n",
       "[26 rows x 9 columns]"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[100]\n",
    "windows1=windows2[...,np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "train_data = windows1[:int(l*.75)]\n",
    "val_data = windows1[int(l*.75):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_48\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000014?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39madd(Flatten())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000014?line=10'>11</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m100\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000014?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39;49madd(LSTM(\u001b[39m24\u001b[39;49m,return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000014?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dropout(\u001b[39m.1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000014?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(\u001b[39m32\u001b[39m,return_sequences\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)),\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/training/tracking/base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    588\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    589\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py:214\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    212\u001b[0m   ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[1;32m    213\u001b[0m   \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[0;32m--> 214\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    215\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    216\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    217\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m   ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm_48\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 100)"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D,LSTM, ConvLSTM1D,Flatten,SimpleRNN,GRU,AveragePooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(50,2,activation='relu',input_shape=train_data[0].shape))\n",
    "model.add(Conv2D(50,2,activation='relu',))\n",
    "model.add(AveragePooling2D(2))\n",
    "model.add(LSTM(24,return_sequences=1,)),\n",
    "model.add(Dropout(.1))\n",
    "model.add(LSTM(32,return_sequences=0)),\n",
    "model.add(Dropout(.1))\n",
    "\n",
    "model.add(Dense(3))\n",
    "\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,batch_size=64,verbose=2 , epochs = 100,validation_data=(val_data,val_labels))\n",
    "\n",
    "# model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "10/10 - 3s - loss: 1.0494 - accuracy: 0.4479 - val_loss: 0.9553 - val_accuracy: 0.5231 - 3s/epoch - 348ms/step\n",
      "Epoch 17/30\n",
      "10/10 - 1s - loss: 1.0421 - accuracy: 0.4256 - val_loss: 0.9514 - val_accuracy: 0.5436 - 570ms/epoch - 57ms/step\n",
      "Epoch 18/30\n",
      "10/10 - 1s - loss: 1.0388 - accuracy: 0.4410 - val_loss: 0.9500 - val_accuracy: 0.5436 - 528ms/epoch - 53ms/step\n",
      "Epoch 19/30\n",
      "10/10 - 1s - loss: 1.0469 - accuracy: 0.4120 - val_loss: 0.9529 - val_accuracy: 0.5436 - 525ms/epoch - 53ms/step\n",
      "Epoch 20/30\n",
      "10/10 - 1s - loss: 1.0315 - accuracy: 0.4444 - val_loss: 1.0045 - val_accuracy: 0.5385 - 527ms/epoch - 53ms/step\n",
      "Epoch 21/30\n",
      "10/10 - 1s - loss: 1.0268 - accuracy: 0.4513 - val_loss: 1.0024 - val_accuracy: 0.5436 - 536ms/epoch - 54ms/step\n",
      "Epoch 22/30\n",
      "10/10 - 1s - loss: 1.0137 - accuracy: 0.4615 - val_loss: 1.0078 - val_accuracy: 0.5282 - 519ms/epoch - 52ms/step\n",
      "Epoch 23/30\n",
      "10/10 - 1s - loss: 1.0451 - accuracy: 0.4359 - val_loss: 0.9964 - val_accuracy: 0.5436 - 519ms/epoch - 52ms/step\n",
      "Epoch 24/30\n",
      "10/10 - 1s - loss: 1.0300 - accuracy: 0.4701 - val_loss: 0.9931 - val_accuracy: 0.5333 - 513ms/epoch - 51ms/step\n",
      "Epoch 25/30\n",
      "10/10 - 1s - loss: 1.0144 - accuracy: 0.4547 - val_loss: 0.9959 - val_accuracy: 0.5487 - 520ms/epoch - 52ms/step\n",
      "Epoch 26/30\n",
      "10/10 - 1s - loss: 1.0487 - accuracy: 0.4735 - val_loss: 1.0015 - val_accuracy: 0.5436 - 511ms/epoch - 51ms/step\n",
      "Epoch 27/30\n",
      "10/10 - 1s - loss: 1.0154 - accuracy: 0.4667 - val_loss: 1.0218 - val_accuracy: 0.5282 - 518ms/epoch - 52ms/step\n",
      "Epoch 28/30\n",
      "10/10 - 1s - loss: 1.0079 - accuracy: 0.4872 - val_loss: 1.0784 - val_accuracy: 0.5282 - 526ms/epoch - 53ms/step\n",
      "Epoch 29/30\n",
      "10/10 - 1s - loss: 1.0189 - accuracy: 0.4479 - val_loss: 1.0326 - val_accuracy: 0.5385 - 538ms/epoch - 54ms/step\n",
      "Epoch 30/30\n",
      "10/10 - 1s - loss: 1.0073 - accuracy: 0.4530 - val_loss: 1.0730 - val_accuracy: 0.5436 - 524ms/epoch - 52ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x385625070>"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,batch_size=128,verbose=2 , initial_epoch=15, epochs = 30,validation_data=(val_data,val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 32, 4, 1)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.345085</td>\n",
       "      <td>0.198083</td>\n",
       "      <td>0.174221</td>\n",
       "      <td>0.247059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.306065</td>\n",
       "      <td>0.073482</td>\n",
       "      <td>0.059490</td>\n",
       "      <td>0.182353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.310232</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.143059</td>\n",
       "      <td>0.110240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.478205</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.165251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.333600</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.032578</td>\n",
       "      <td>0.155991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.196083</td>\n",
       "      <td>0.054313</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.156972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.379737</td>\n",
       "      <td>0.230032</td>\n",
       "      <td>0.155807</td>\n",
       "      <td>0.211656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.422896</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.228045</td>\n",
       "      <td>0.198366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.300855</td>\n",
       "      <td>0.075080</td>\n",
       "      <td>0.055241</td>\n",
       "      <td>0.151525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.295173</td>\n",
       "      <td>0.051118</td>\n",
       "      <td>0.067989</td>\n",
       "      <td>0.130501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.372293</td>\n",
       "      <td>0.130990</td>\n",
       "      <td>0.035411</td>\n",
       "      <td>0.131590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.521840</td>\n",
       "      <td>0.134185</td>\n",
       "      <td>0.029745</td>\n",
       "      <td>0.136710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.716397</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.041076</td>\n",
       "      <td>0.373203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.504749</td>\n",
       "      <td>0.498403</td>\n",
       "      <td>0.011331</td>\n",
       "      <td>0.469063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.360410</td>\n",
       "      <td>0.571885</td>\n",
       "      <td>0.070822</td>\n",
       "      <td>0.360893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.349953</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>0.461756</td>\n",
       "      <td>0.317320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.440161</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.110482</td>\n",
       "      <td>0.211002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.521459</td>\n",
       "      <td>0.110224</td>\n",
       "      <td>0.203966</td>\n",
       "      <td>0.229085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.431596</td>\n",
       "      <td>0.305112</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>0.246514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.320172</td>\n",
       "      <td>0.322684</td>\n",
       "      <td>0.151558</td>\n",
       "      <td>0.258061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.278929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130312</td>\n",
       "      <td>0.265359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.480925</td>\n",
       "      <td>0.108626</td>\n",
       "      <td>0.116147</td>\n",
       "      <td>0.234858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.478929</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.063739</td>\n",
       "      <td>0.122331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.315559</td>\n",
       "      <td>0.023962</td>\n",
       "      <td>0.079320</td>\n",
       "      <td>0.146623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.420170</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.094901</td>\n",
       "      <td>0.123312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.500770</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.031161</td>\n",
       "      <td>0.079739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.375746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3\n",
       "0   0.345085  0.198083  0.174221  0.247059\n",
       "1   0.306065  0.073482  0.059490  0.182353\n",
       "2   0.375746  0.000000  0.000000  0.000000\n",
       "3   0.375746  0.000000  0.000000  0.000000\n",
       "4   0.375746  0.000000  0.000000  0.000000\n",
       "5   0.375746  0.000000  0.000000  0.000000\n",
       "6   0.375746  0.000000  0.000000  0.000000\n",
       "7   0.310232  0.011182  0.143059  0.110240\n",
       "8   0.478205  0.028754  0.000000  0.165251\n",
       "9   0.333600  0.049521  0.032578  0.155991\n",
       "10  0.196083  0.054313  0.016997  0.156972\n",
       "11  0.379737  0.230032  0.155807  0.211656\n",
       "12  0.422896  0.014377  0.228045  0.198366\n",
       "13  0.300855  0.075080  0.055241  0.151525\n",
       "14  0.295173  0.051118  0.067989  0.130501\n",
       "15  0.372293  0.130990  0.035411  0.131590\n",
       "16  0.521840  0.134185  0.029745  0.136710\n",
       "17  0.716397  0.020767  0.041076  0.373203\n",
       "18  0.504749  0.498403  0.011331  0.469063\n",
       "19  0.360410  0.571885  0.070822  0.360893\n",
       "20  0.349953  0.022364  0.461756  0.317320\n",
       "21  0.440161  0.012780  0.110482  0.211002\n",
       "22  0.521459  0.110224  0.203966  0.229085\n",
       "23  0.431596  0.305112  0.015581  0.246514\n",
       "24  0.320172  0.322684  0.151558  0.258061\n",
       "25  0.278929  0.000000  0.130312  0.265359\n",
       "26  0.480925  0.108626  0.116147  0.234858\n",
       "27  0.478929  0.007987  0.063739  0.122331\n",
       "28  0.315559  0.023962  0.079320  0.146623\n",
       "29  0.420170  0.011182  0.094901  0.123312\n",
       "30  0.500770  0.028754  0.031161  0.079739\n",
       "31  0.375746  0.000000  0.000000  0.000109"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.squeeze(train_data[232]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 28, 3, 32)         352       \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 24, 2, 32)         10272     \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 20, 1, 32)         10272     \n",
      "                                                                 \n",
      " flatten_25 (Flatten)        (None, 640)               0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 3)                 1923      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,819\n",
      "Trainable params: 22,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model_1 = tf.keras.Sequential([\n",
    "    keras.layers.Conv2D(32,(5,2) , input_shape=train_data[0].shape,activation='relu'),\n",
    "    keras.layers.Conv2D(32,(5,2) ,activation='relu'),\n",
    "    keras.layers.Conv2D(32,(5,2) ,activation='relu'),\n",
    "\n",
    " \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(3 ,activation='softmax' ),\n",
    "])\n",
    "model_1.compile(\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model_1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "585/585 - 4s - loss: 1.0849 - accuracy: 0.3590 - val_loss: 1.1960 - val_accuracy: 0.3436 - 4s/epoch - 6ms/step\n",
      "Epoch 2/250\n",
      "585/585 - 3s - loss: 1.0347 - accuracy: 0.4325 - val_loss: 1.2065 - val_accuracy: 0.3846 - 3s/epoch - 5ms/step\n",
      "Epoch 3/250\n",
      "585/585 - 3s - loss: 0.9637 - accuracy: 0.5111 - val_loss: 1.0973 - val_accuracy: 0.5538 - 3s/epoch - 5ms/step\n",
      "Epoch 4/250\n",
      "585/585 - 3s - loss: 0.9664 - accuracy: 0.4923 - val_loss: 1.2128 - val_accuracy: 0.4000 - 3s/epoch - 5ms/step\n",
      "Epoch 5/250\n",
      "585/585 - 3s - loss: 0.9543 - accuracy: 0.4872 - val_loss: 1.2458 - val_accuracy: 0.4000 - 3s/epoch - 5ms/step\n",
      "Epoch 6/250\n",
      "585/585 - 3s - loss: 0.9461 - accuracy: 0.5111 - val_loss: 1.1070 - val_accuracy: 0.5385 - 3s/epoch - 5ms/step\n",
      "Epoch 7/250\n",
      "585/585 - 3s - loss: 0.9429 - accuracy: 0.5145 - val_loss: 1.1544 - val_accuracy: 0.5436 - 3s/epoch - 5ms/step\n",
      "Epoch 8/250\n",
      "585/585 - 3s - loss: 0.9486 - accuracy: 0.4872 - val_loss: 1.1276 - val_accuracy: 0.3641 - 3s/epoch - 5ms/step\n",
      "Epoch 9/250\n",
      "585/585 - 3s - loss: 0.9343 - accuracy: 0.5043 - val_loss: 1.3347 - val_accuracy: 0.3949 - 3s/epoch - 5ms/step\n",
      "Epoch 10/250\n",
      "585/585 - 3s - loss: 0.9358 - accuracy: 0.5145 - val_loss: 1.2349 - val_accuracy: 0.4974 - 3s/epoch - 5ms/step\n",
      "Epoch 11/250\n",
      "585/585 - 3s - loss: 0.9346 - accuracy: 0.5094 - val_loss: 1.6134 - val_accuracy: 0.3744 - 3s/epoch - 5ms/step\n",
      "Epoch 12/250\n",
      "585/585 - 3s - loss: 0.9293 - accuracy: 0.5179 - val_loss: 1.3150 - val_accuracy: 0.4154 - 3s/epoch - 5ms/step\n",
      "Epoch 13/250\n",
      "585/585 - 3s - loss: 0.9315 - accuracy: 0.4991 - val_loss: 1.6456 - val_accuracy: 0.3846 - 3s/epoch - 5ms/step\n",
      "Epoch 14/250\n",
      "585/585 - 3s - loss: 0.9298 - accuracy: 0.5197 - val_loss: 1.3335 - val_accuracy: 0.4000 - 3s/epoch - 5ms/step\n",
      "Epoch 15/250\n",
      "585/585 - 3s - loss: 0.9244 - accuracy: 0.5248 - val_loss: 1.3861 - val_accuracy: 0.4205 - 3s/epoch - 5ms/step\n",
      "Epoch 16/250\n",
      "585/585 - 3s - loss: 0.9214 - accuracy: 0.5179 - val_loss: 1.4806 - val_accuracy: 0.4667 - 3s/epoch - 5ms/step\n",
      "Epoch 17/250\n",
      "585/585 - 3s - loss: 0.9282 - accuracy: 0.5179 - val_loss: 1.5445 - val_accuracy: 0.4974 - 3s/epoch - 4ms/step\n",
      "Epoch 18/250\n",
      "585/585 - 3s - loss: 0.9289 - accuracy: 0.5214 - val_loss: 1.5630 - val_accuracy: 0.3795 - 3s/epoch - 5ms/step\n",
      "Epoch 19/250\n",
      "585/585 - 3s - loss: 0.9270 - accuracy: 0.5231 - val_loss: 1.5967 - val_accuracy: 0.5179 - 3s/epoch - 5ms/step\n",
      "Epoch 20/250\n",
      "585/585 - 3s - loss: 0.9257 - accuracy: 0.5504 - val_loss: 1.5552 - val_accuracy: 0.3846 - 3s/epoch - 5ms/step\n",
      "Epoch 21/250\n",
      "585/585 - 3s - loss: 0.9211 - accuracy: 0.5179 - val_loss: 1.7228 - val_accuracy: 0.5333 - 3s/epoch - 5ms/step\n",
      "Epoch 22/250\n",
      "585/585 - 3s - loss: 0.9169 - accuracy: 0.5470 - val_loss: 1.6266 - val_accuracy: 0.4821 - 3s/epoch - 5ms/step\n",
      "Epoch 23/250\n",
      "585/585 - 3s - loss: 0.9118 - accuracy: 0.5265 - val_loss: 1.4844 - val_accuracy: 0.5231 - 3s/epoch - 5ms/step\n",
      "Epoch 24/250\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000021?line=0'>1</a>\u001b[0m model_1_history \u001b[39m=\u001b[39m model_1\u001b[39m.\u001b[39;49mfit( train_data, train_labels,batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m ,verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,epochs\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(val_data,val_labels),shuffle\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model_1_history = model_1.fit( train_data, train_labels,batch_size=1 ,verbose=2,epochs=250,validation_data=(val_data,val_labels),shuffle=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_squeeze=np.squeeze(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=np.squeeze(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 32, 4)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_squeeze.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "585/585 - 4s - loss: 1.0854 - accuracy: 0.3761 - val_loss: 0.9521 - val_accuracy: 0.4205 - 4s/epoch - 6ms/step\n",
      "Epoch 2/200\n",
      "585/585 - 3s - loss: 1.0036 - accuracy: 0.4838 - val_loss: 0.9410 - val_accuracy: 0.3795 - 3s/epoch - 5ms/step\n",
      "Epoch 3/200\n",
      "585/585 - 3s - loss: 0.9677 - accuracy: 0.4940 - val_loss: 0.9477 - val_accuracy: 0.3897 - 3s/epoch - 5ms/step\n",
      "Epoch 4/200\n",
      "585/585 - 3s - loss: 0.9646 - accuracy: 0.4821 - val_loss: 0.9007 - val_accuracy: 0.5385 - 3s/epoch - 5ms/step\n",
      "Epoch 5/200\n",
      "585/585 - 3s - loss: 0.9583 - accuracy: 0.5026 - val_loss: 0.9162 - val_accuracy: 0.5436 - 3s/epoch - 5ms/step\n",
      "Epoch 6/200\n",
      "585/585 - 3s - loss: 0.9550 - accuracy: 0.5060 - val_loss: 0.9249 - val_accuracy: 0.4667 - 3s/epoch - 5ms/step\n",
      "Epoch 7/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000022?line=15'>16</a>\u001b[0m model_2\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39msparse_categorical_crossentropy,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000022?line=16'>17</a>\u001b[0m optimizer \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000022?line=17'>18</a>\u001b[0m metrics \u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000022?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000022?line=20'>21</a>\u001b[0m \u001b[39m# model_2.summary()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000022?line=22'>23</a>\u001b[0m model_2_history \u001b[39m=\u001b[39m model_2\u001b[39m.\u001b[39;49mfit( train_data, train_labels ,batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m , epochs \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m,shuffle\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(val_data,val_labels))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(14)\n",
    "\n",
    "model_2 = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten( input_shape=windows[0].shape),\n",
    "\n",
    "    tf.keras.layers.Dense(50 ,activation='relu' ),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(50 ,activation='relu' ),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(50 ,activation='relu' ),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3 ,activation='softmax' ),\n",
    "])\n",
    "model_2.compile(loss = tf.keras.losses.sparse_categorical_crossentropy,\n",
    "optimizer = keras.optimizers.Adam(),\n",
    "metrics =['accuracy']\n",
    ")\n",
    "\n",
    "# model_2.summary()\n",
    "\n",
    "model_2_history = model_2.fit( train_data, train_labels ,batch_size=1,verbose=2 , epochs = 200,shuffle=1,validation_data=(val_data,val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.010226</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.180084</td>\n",
       "      <td>0.577697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.024867</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.039542</td>\n",
       "      <td>0.174547</td>\n",
       "      <td>0.570750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461322</td>\n",
       "      <td>0.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445945</td>\n",
       "      <td>0.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431080</td>\n",
       "      <td>0.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416711</td>\n",
       "      <td>0.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402820</td>\n",
       "      <td>0.053014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.006585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.681964</td>\n",
       "      <td>0.587567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.012447</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.026912</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>0.659652</td>\n",
       "      <td>0.587321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.005859</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.008499</td>\n",
       "      <td>0.014379</td>\n",
       "      <td>0.637868</td>\n",
       "      <td>0.587218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.009526</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.024079</td>\n",
       "      <td>0.030937</td>\n",
       "      <td>0.617049</td>\n",
       "      <td>0.587029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.010264</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.015581</td>\n",
       "      <td>0.026362</td>\n",
       "      <td>0.596900</td>\n",
       "      <td>0.586810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.020550</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>0.034641</td>\n",
       "      <td>0.577492</td>\n",
       "      <td>0.586288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.006608</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.031161</td>\n",
       "      <td>0.034423</td>\n",
       "      <td>0.558628</td>\n",
       "      <td>0.586095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.000734</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.540212</td>\n",
       "      <td>0.586074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.012475</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>0.522647</td>\n",
       "      <td>0.586327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.002202</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>0.011331</td>\n",
       "      <td>0.022004</td>\n",
       "      <td>0.505407</td>\n",
       "      <td>0.586256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>0.017320</td>\n",
       "      <td>0.488765</td>\n",
       "      <td>0.586310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.002202</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.472586</td>\n",
       "      <td>0.586256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.013181</td>\n",
       "      <td>0.456935</td>\n",
       "      <td>0.586298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.015422</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.012748</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.442113</td>\n",
       "      <td>0.585643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.002938</td>\n",
       "      <td>0.011182</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.427637</td>\n",
       "      <td>0.585543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.022060</td>\n",
       "      <td>0.004792</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.028322</td>\n",
       "      <td>0.413768</td>\n",
       "      <td>0.584472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.019875</td>\n",
       "      <td>0.006390</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.400351</td>\n",
       "      <td>0.583444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.015974</td>\n",
       "      <td>0.042493</td>\n",
       "      <td>0.045098</td>\n",
       "      <td>0.387471</td>\n",
       "      <td>0.583532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.019169</td>\n",
       "      <td>0.033994</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>0.375157</td>\n",
       "      <td>0.583910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.017677</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.026912</td>\n",
       "      <td>0.044771</td>\n",
       "      <td>0.363208</td>\n",
       "      <td>0.582557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.000737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.351533</td>\n",
       "      <td>0.582458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.033945</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.038889</td>\n",
       "      <td>0.340383</td>\n",
       "      <td>0.579958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.018466</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.012748</td>\n",
       "      <td>0.037908</td>\n",
       "      <td>0.329513</td>\n",
       "      <td>0.578544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.010336</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>0.043246</td>\n",
       "      <td>0.319120</td>\n",
       "      <td>0.579123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.004431</td>\n",
       "      <td>0.022364</td>\n",
       "      <td>0.012748</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>0.308812</td>\n",
       "      <td>0.578670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5\n",
       "0  -0.010226  0.015974  0.014164  0.037255  0.180084  0.577697\n",
       "1  -0.024867  0.009585  0.001416  0.039542  0.174547  0.570750\n",
       "2   0.000000  0.000000  0.000000  0.000000  0.461322  0.053014\n",
       "3   0.000000  0.000000  0.000000  0.000000  0.445945  0.053014\n",
       "4   0.000000  0.000000  0.000000  0.000000  0.431080  0.053014\n",
       "5   0.000000  0.000000  0.000000  0.000000  0.416711  0.053014\n",
       "6   0.000000  0.000000  0.000000  0.000000  0.402820  0.053014\n",
       "7  -0.006585  0.000000  0.002833  0.008170  0.681964  0.587567\n",
       "8  -0.012447  0.001597  0.026912  0.015904  0.659652  0.587321\n",
       "9  -0.005859  0.006390  0.008499  0.014379  0.637868  0.587218\n",
       "10 -0.009526  0.014377  0.024079  0.030937  0.617049  0.587029\n",
       "11 -0.010264  0.019169  0.015581  0.026362  0.596900  0.586810\n",
       "12 -0.020550  0.001597  0.019830  0.034641  0.577492  0.586288\n",
       "13 -0.006608  0.004792  0.031161  0.034423  0.558628  0.586095\n",
       "14 -0.000734  0.004792  0.019830  0.025599  0.540212  0.586074\n",
       "15  0.012475  0.028754  0.005666  0.026688  0.522647  0.586327\n",
       "16 -0.002202  0.007987  0.011331  0.022004  0.505407  0.586256\n",
       "17  0.001468  0.009585  0.014164  0.017320  0.488765  0.586310\n",
       "18 -0.002202  0.003195  0.007082  0.013617  0.472586  0.586256\n",
       "19  0.001468  0.000000  0.009915  0.013181  0.456935  0.586298\n",
       "20 -0.015422  0.009585  0.012748  0.025599  0.442113  0.585643\n",
       "21 -0.002938  0.011182  0.016997  0.019499  0.427637  0.585543\n",
       "22 -0.022060  0.004792  0.001416  0.028322  0.413768  0.584472\n",
       "23 -0.019875  0.006390  0.002833  0.031373  0.400351  0.583444\n",
       "24  0.000736  0.015974  0.042493  0.045098  0.387471  0.583532\n",
       "25  0.012507  0.019169  0.033994  0.046296  0.375157  0.583910\n",
       "26 -0.017677  0.009585  0.026912  0.044771  0.363208  0.582557\n",
       "27 -0.000737  0.000000  0.050992  0.044444  0.351533  0.582458\n",
       "28 -0.033945  0.003195  0.002833  0.038889  0.340383  0.579958\n",
       "29 -0.018466  0.012780  0.012748  0.037908  0.329513  0.578544\n",
       "30  0.010336  0.003195  0.050992  0.043246  0.319120  0.579123\n",
       "31 -0.004431  0.022364  0.012748  0.032135  0.308812  0.578670"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.squeeze(train_data[23]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "19/19 - 0s - loss: 1.5990 - accuracy: 0.3761 - 404ms/epoch - 21ms/step\n",
      "Epoch 2/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 103ms/epoch - 5ms/step\n",
      "Epoch 3/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 106ms/epoch - 6ms/step\n",
      "Epoch 4/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 124ms/epoch - 7ms/step\n",
      "Epoch 5/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 103ms/epoch - 5ms/step\n",
      "Epoch 6/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 105ms/epoch - 6ms/step\n",
      "Epoch 7/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 103ms/epoch - 5ms/step\n",
      "Epoch 8/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 94ms/epoch - 5ms/step\n",
      "Epoch 9/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 95ms/epoch - 5ms/step\n",
      "Epoch 10/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 95ms/epoch - 5ms/step\n",
      "Epoch 11/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 96ms/epoch - 5ms/step\n",
      "Epoch 12/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 96ms/epoch - 5ms/step\n",
      "Epoch 13/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 104ms/epoch - 5ms/step\n",
      "Epoch 14/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 97ms/epoch - 5ms/step\n",
      "Epoch 15/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 96ms/epoch - 5ms/step\n",
      "Epoch 16/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 98ms/epoch - 5ms/step\n",
      "Epoch 17/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 97ms/epoch - 5ms/step\n",
      "Epoch 18/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 99ms/epoch - 5ms/step\n",
      "Epoch 19/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 98ms/epoch - 5ms/step\n",
      "Epoch 20/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 99ms/epoch - 5ms/step\n",
      "Epoch 21/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 101ms/epoch - 5ms/step\n",
      "Epoch 22/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 98ms/epoch - 5ms/step\n",
      "Epoch 23/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 97ms/epoch - 5ms/step\n",
      "Epoch 24/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 95ms/epoch - 5ms/step\n",
      "Epoch 25/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 95ms/epoch - 5ms/step\n",
      "Epoch 26/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 101ms/epoch - 5ms/step\n",
      "Epoch 27/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 97ms/epoch - 5ms/step\n",
      "Epoch 28/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 98ms/epoch - 5ms/step\n",
      "Epoch 29/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 97ms/epoch - 5ms/step\n",
      "Epoch 30/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 96ms/epoch - 5ms/step\n",
      "Epoch 31/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 96ms/epoch - 5ms/step\n",
      "Epoch 32/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 96ms/epoch - 5ms/step\n",
      "Epoch 33/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 103ms/epoch - 5ms/step\n",
      "Epoch 34/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 94ms/epoch - 5ms/step\n",
      "Epoch 35/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 101ms/epoch - 5ms/step\n",
      "Epoch 36/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 118ms/epoch - 6ms/step\n",
      "Epoch 37/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 107ms/epoch - 6ms/step\n",
      "Epoch 38/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 96ms/epoch - 5ms/step\n",
      "Epoch 39/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 97ms/epoch - 5ms/step\n",
      "Epoch 40/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 101ms/epoch - 5ms/step\n",
      "Epoch 41/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 104ms/epoch - 5ms/step\n",
      "Epoch 42/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 95ms/epoch - 5ms/step\n",
      "Epoch 43/200\n",
      "19/19 - 0s - loss: 1.0986 - accuracy: 0.3778 - 95ms/epoch - 5ms/step\n",
      "Epoch 44/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alint/Desktop/CTFlearn/digi/PyCourse/Cnn.ipynb#ch0000023?line=0'>1</a>\u001b[0m model_2_history \u001b[39m=\u001b[39m model_2\u001b[39m.\u001b[39;49mfit( train_data, train_labels ,verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m , epochs \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m,shuffle\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee3717197db56dab91ad083a26bef10706ce761f0ab8e349ac843a6f8d1f4192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
